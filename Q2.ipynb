{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/mohsenperfection/Test/blob/master/Q2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ChC3RF8meAlK"
      },
      "source": [
        "<h1 align=\"center\">Introduction to Machine Learning - 25737-2</h1>\n",
        "<h4 align=\"center\">Dr. R. Amiri</h4>\n",
        "<h4 align=\"center\">Sharif University of Technology, Spring 2024</h4>\n",
        "\n",
        "\n",
        "**<font color='red'>Plagiarism is strongly prohibited!</font>**\n",
        "\n",
        "\n",
        "**Student Name**: **Mohsen Kamalabadi Farahani**\n",
        "\n",
        "**Student ID**: **99102083**\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IraiR0SbeDi_"
      },
      "source": [
        "# Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nRQjwWC3eDnc"
      },
      "source": [
        "**Task:** Implement your own Logistic Regression model, and test it on the given dataset of Logistic_question.csv!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "1eRP5ZaM9UjD"
      },
      "outputs": [],
      "source": [
        "# import necessary libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split, KFold\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
        "import matplotlib.pyplot as plt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hIR6qFfx9UjF"
      },
      "outputs": [],
      "source": [
        "class MyLogisticRegression:\n",
        "    # Your code goes here!\n",
        "\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=10000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        \"\"\"Apply the sigmoid function.\"\"\"\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def _initialize_parameters(self, n_features):\n",
        "        \"\"\"Initialize weights and bias to zeros (can also initialize randomly).\"\"\"\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "    def _log_loss(self, y_true, y_pred):\n",
        "        \"\"\"Compute the logistic loss.\"\"\"\n",
        "        m = y_true.shape[0]\n",
        "        loss = -np.sum(y_true * np.log(y_pred) + (1 - y_true) * np.log(1 - y_pred)) / m\n",
        "        return loss\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        \"\"\"Fit the logistic regression model using gradient descent.\"\"\"\n",
        "        n_samples, n_features = X.shape\n",
        "        self._initialize_parameters(n_features)\n",
        "\n",
        "        # Gradient descent\n",
        "        for i in range(self.num_iterations):\n",
        "            # Linear model\n",
        "            z = np.dot(X, self.weights) + self.bias\n",
        "            y_pred = self._sigmoid(z)\n",
        "\n",
        "            # Compute gradients\n",
        "            dw = np.dot(X.T, (y_pred - y)) / n_samples\n",
        "            db = np.sum(y_pred - y) / n_samples\n",
        "\n",
        "            # Update parameters\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "            # Optionally print loss (can slow down the computation)\n",
        "            if i % 1000 == 0:\n",
        "                loss = self._log_loss(y, y_pred)\n",
        "                print(f\"Iteration {i}, Loss: {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        \"\"\"Predict using the logistic model.\"\"\"\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        y_pred = self._sigmoid(z)\n",
        "        return [1 if i > 0.5 else 0 for i in y_pred]\n",
        "\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S-i-oubUlZ6e"
      },
      "source": [
        "**Task:** Test your model on the given dataset. You must split your data into train and test, with a 0.2 split, then normalize your data using X_train data. Finally, report 4 different evaluation metrics of the model on the test set. (You might want to first make the Target column binary!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0KXzIy_2u-pG",
        "outputId": "9625f7e2-abb1-4591-c0fa-843525e0ffd6"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Iteration 0, Loss: 0.6931\n",
            "Iteration 1000, Loss: 0.2355\n",
            "Iteration 2000, Loss: 0.1922\n",
            "Iteration 3000, Loss: 0.1757\n",
            "Iteration 4000, Loss: 0.1667\n",
            "Iteration 5000, Loss: 0.1609\n",
            "Iteration 6000, Loss: 0.1569\n",
            "Iteration 7000, Loss: 0.1540\n",
            "Iteration 8000, Loss: 0.1518\n",
            "Iteration 9000, Loss: 0.1501\n",
            "Accuracy: 0.9375\n",
            "Precision: 0.9333\n",
            "Recall: 1.0000\n",
            "F1 Score: 0.9655\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "\n",
        "# Load the dataset\n",
        "data = pd.read_csv('Q2/Logistic_question.csv')\n",
        "\n",
        "# Convert Target to binary\n",
        "data['Target'] = (data['Target'] > 0.5).astype(int)\n",
        "\n",
        "# Features and target\n",
        "X = data.drop('Target', axis=1)\n",
        "y = data['Target']\n",
        "\n",
        "# Train and test sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Normalize features based on X_train\n",
        "mean = X_train.mean()\n",
        "std = X_train.std()\n",
        "\n",
        "X_train = (X_train - mean) / std\n",
        "X_test = (X_test - mean) / std\n",
        "\n",
        "# Convert to numpy arrays\n",
        "X_train = X_train.values\n",
        "X_test = X_test.values\n",
        "y_train = y_train.values\n",
        "y_test = y_test.values\n",
        "\n",
        "# Train the model\n",
        "model = MyLogisticRegression(learning_rate=0.01, num_iterations=10000)\n",
        "model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred = model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy = accuracy_score(y_test, y_pred)\n",
        "precision = precision_score(y_test, y_pred)\n",
        "recall = recall_score(y_test, y_pred)\n",
        "f1 = f1_score(y_test, y_pred)\n",
        "\n",
        "print(f\"Accuracy: {accuracy:.4f}\")\n",
        "print(f\"Precision: {precision:.4f}\")\n",
        "print(f\"Recall: {recall:.4f}\")\n",
        "print(f\"F1 Score: {f1:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ji0RXNGKv1pa"
      },
      "source": [
        "**Question:** What are each of your used evaluation metrics? And for each one, mention situations in which they convey more data on the model performance in specific tasks."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ldveD35twRRZ"
      },
      "source": [
        "**Your answer:**\n",
        "\n",
        "Evaluation metrics are crucial for assessing the performance of classification models, each highlighting different aspects of model behavior. Here’s an overview of the four metrics used: accuracy, precision, recall, and F1 score, along with contexts where each metric is particularly informative:\n",
        "\n",
        "### 1. **Accuracy**\n",
        "**Definition:** Accuracy measures the proportion of total correct predictions (both true positives and true negatives) out of all predictions made. It is calculated as:\n",
        "\n",
        "\n",
        "**Relevant Contexts:**\n",
        "- Balanced Classes: Accuracy is most informative when the classes in the dataset are roughly equally represented.\n",
        "- General Overview: It gives a quick, intuitive understanding of the overall effectiveness of a model across all classes.\n",
        "\n",
        "### 2. **Precision**\n",
        "**Definition:** Precision measures the accuracy of positive predictions. Formally, it is the ratio of true positives to all predicted positives. It is calculated as:\n",
        "\n",
        "\n",
        "**Relevant Contexts:**\n",
        "- Low Tolerance for False Positives: Precision is critical in scenarios where the cost of a false positive is high. For example, in email spam detection, a high precision model minimizes the risk of classifying important emails as spam.\n",
        "- Financial Fraud Detection: Ensuring that legitimate transactions are not flagged unnecessarily.\n",
        "\n",
        "### 3. **Recall (Sensitivity)**\n",
        "**Definition:** Recall measures the model's ability to detect positive instances. It is the ratio of true positives to the sum of true positives and false negatives:\n",
        "\n",
        "**Relevant Contexts:**\n",
        "- Low Tolerance for False Negatives: Crucial in medical scenarios like cancer detection, where failing to diagnose a sick patient (false negative) is far worse than incorrectly diagnosing wellness (false positive).\n",
        "- Law Enforcement: Missing a positive case (e.g., not catching a criminal) can be more severe than a false alarm.\n",
        "\n",
        "### 4. **F1 Score**\n",
        "**Definition:** The F1 score is the harmonic mean of precision and recall, providing a single metric that balances both. It’s especially useful when you need to balance precision and recall. It is calculated as:\n",
        "\n",
        "**Relevant Contexts:**\n",
        "- Unbalanced Classes: Particularly effective in scenarios where one class is significantly underrepresented.\n",
        "- Trade-off Situations: Ideal when there is a need to find a balance between precision and recall, such as in document classification or customer support ticket tagging, where both over-tagging and under-tagging are undesirable."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1ZCeRHZSw-mh"
      },
      "source": [
        "**Task:** Now test the built-in function of Python for Logistic Regression, and report all the same metrics used before."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vb5lRSQXDLR3",
        "outputId": "7f187bcd-4b7b-42d7-a42d-972b0c03ba22"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Evaluation Metrics for scikit-learn Logistic Regression:\n",
            "Accuracy: 0.9500\n",
            "Precision: 0.9459\n",
            "Recall: 1.0000\n",
            "F1 Score: 0.9722\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "\n",
        "# A logistic regression model\n",
        "sklearn_model = LogisticRegression()\n",
        "\n",
        "# Train the model\n",
        "sklearn_model.fit(X_train, y_train)\n",
        "\n",
        "# Predict on the test set\n",
        "y_pred_sklearn = sklearn_model.predict(X_test)\n",
        "\n",
        "# Evaluate the model\n",
        "accuracy_sklearn = accuracy_score(y_test, y_pred_sklearn)\n",
        "precision_sklearn = precision_score(y_test, y_pred_sklearn)\n",
        "recall_sklearn = recall_score(y_test, y_pred_sklearn)\n",
        "f1_sklearn = f1_score(y_test, y_pred_sklearn)\n",
        "\n",
        "print(\"Evaluation Metrics for scikit-learn Logistic Regression:\")\n",
        "print(f\"Accuracy: {accuracy_sklearn:.4f}\")\n",
        "print(f\"Precision: {precision_sklearn:.4f}\")\n",
        "print(f\"Recall: {recall_sklearn:.4f}\")\n",
        "print(f\"F1 Score: {f1_sklearn:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RCvIymmMy_ji"
      },
      "source": [
        "**Question:** Compare your function with the built-in function. On the matters of performance and parameters. Briefly explain what the parameters of the built-in function are and how they affect the model's performance?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EY0ohM16z3De"
      },
      "source": [
        "**Your answer:**\n",
        "### Comparison: Custom Function vs. Built-in Function\n",
        "\n",
        "**Performance:**\n",
        "1. **Accuracy and Convergence:**\n",
        "   - The built-in function from scikit-learn (`LogisticRegression`) is generally more robust and optimized than a simple custom implementation. It uses advanced optimization techniques (like the LIBLINEAR solver for small datasets or SAG and LBFGS for larger ones) that help in achieving better convergence and accuracy.\n",
        "   - The custom model may not converge as efficiently and could be more sensitive to the choice of learning rate and number of iterations due to its simpler gradient descent implementation.\n",
        "\n",
        "2. **Feature Handling:**\n",
        "   - Scikit-learn's logistic regression can handle a variety of numeric and categorical input features more robustly, implementing internal mechanisms for dealing with issues like multicollinearity through regularization.\n",
        "   - The custom model lacks any form of regularization, making it potentially less effective on real-world datasets where multicollinearity or feature scaling might be issues.\n",
        "\n",
        "3. **Ease of Use:**\n",
        "   - Scikit-learn provides a lot of automatic features like data scaling, handling of multicollinearity, and choice of different optimization algorithms, which greatly simplify model training and improve performance.\n",
        "   - The custom implementation requires manual setup for aspects like feature scaling and modification of the learning algorithm.\n",
        "\n",
        "**Parameters of Scikit-learn's Logistic Regression and Their Impact:**\n",
        "1. **`penalty`** (default=`'l2'`): Specifies the norm used in the penalization (regularization). Common options are `l1`, `l2`, and `elasticnet`. Regularization can prevent overfitting by penalizing larger coefficients, thus improving model's generalization.\n",
        "2. **`C`** (default=`1.0`): Inverse of regularization strength; smaller values specify stronger regularization. Affects how much you want to penalize the model for larger coefficients, influencing feature importance and model simplicity.\n",
        "3. **`solver`** (default=`'lbfgs'`):\n",
        "   - Options like `'liblinear'`, `'newton-cg'`, `'lbfgs'`, `'sag'` and `'saga'` provide different algorithms for optimization.\n",
        "   - `'liblinear'` is good for small datasets, but doesn’t support `l1` penalty with multinomial loss.\n",
        "   - `'lbfgs'`, `'newton-cg'`, and `'sag'` are suitable for large datasets and support multinomial loss.\n",
        "   - Choice of solver can affect the speed and accuracy of convergence, especially in terms of how they handle large datasets or multinomial outcomes.\n",
        "4. **`max_iter`** (default=`100`): Maximum number of iterations taken for the solvers to converge. Increasing this may lead to better convergence at the cost of computational time, especially relevant in complex models or very large datasets.\n",
        "5. **`multi_class`** (default=`'auto'`): Can be set to `'ovr'` (one-vs-rest) or `'multinomial'` to specify how to handle multiple classes. The choice here can affect the accuracy across different classes in a multi-class problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ClMqoYlr2kr7"
      },
      "source": [
        "# Multinomial Logistic Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ukvlqDe52xP5"
      },
      "source": [
        "**Task:** Implement your own Multinomial Logistic Regression model. Your model must be able to handle any number of labels!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Ir-_hFt286t"
      },
      "outputs": [],
      "source": [
        "class MyMultinomialLogisticRegression:\n",
        "    # Your code goes here!\n",
        "\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def _softmax(self, z):\n",
        "        exp_z = np.exp(z - np.max(z, axis=1, keepdims=True))\n",
        "        return exp_z / np.sum(exp_z, axis=1, keepdims=True)\n",
        "\n",
        "    def _cross_entropy_loss(self, y_true, y_pred):\n",
        "        m = y_true.shape[0]\n",
        "        y_pred = np.clip(y_pred, 1e-15, 1 - 1e-15)\n",
        "        return -np.sum(y_true * np.log(y_pred)) / m\n",
        "\n",
        "    def _initialize_parameters(self, n_features, n_classes):\n",
        "        self.weights = np.zeros((n_features, n_classes))\n",
        "        self.bias = np.zeros((n_classes,))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        n_classes = np.max(y) + 1\n",
        "\n",
        "        # Convert y to one-hot encoding\n",
        "        y_one_hot = np.eye(n_classes)[y]\n",
        "\n",
        "        self._initialize_parameters(n_features, n_classes)\n",
        "\n",
        "        # Gradient descent\n",
        "        for i in range(self.num_iterations):\n",
        "            # Compute predictions\n",
        "            z = np.dot(X, self.weights) + self.bias\n",
        "            y_pred = self._softmax(z)\n",
        "\n",
        "            # The gradient\n",
        "            dw = np.dot(X.T, (y_pred - y_one_hot)) / n_samples\n",
        "            db = np.sum(y_pred - y_one_hot, axis=0) / n_samples\n",
        "\n",
        "            # Update parameters\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "            # Optionally print the loss\n",
        "            if i % 100 == 0:\n",
        "                loss = self._cross_entropy_loss(y_one_hot, y_pred)\n",
        "                print(f\"Iteration {i}: Loss {loss:.4f}\")\n",
        "\n",
        "    def predict(self, X):\n",
        "        z = np.dot(X, self.weights) + self.bias\n",
        "        y_pred = self._softmax(z)\n",
        "        return np.argmax(y_pred, axis=1)\n",
        "    pass"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zPQ3Rtay3Y2_"
      },
      "source": [
        "**Task:** Test your model on the given dataset. Do the same as the previous part, but here you might want to first make the Target column quantized into $i$ levels. Change $i$ from 2 to 10."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9aP4QJPq29B3",
        "outputId": "58e6ceca-436f-4356-98a6-d7b73271c495"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Quantization into 2 levels: bins=[0.33937 0.655   0.97   ]\n",
            "Iteration 0: Loss 0.6931\n",
            "Iteration 100: Loss 0.4371\n",
            "Iteration 200: Loss 0.3910\n",
            "Iteration 300: Loss 0.3672\n",
            "Iteration 400: Loss 0.3525\n",
            "Iteration 500: Loss 0.3426\n",
            "Iteration 600: Loss 0.3356\n",
            "Iteration 700: Loss 0.3305\n",
            "Iteration 800: Loss 0.3266\n",
            "Iteration 900: Loss 0.3235\n",
            "Accuracy with 2 levels: 0.8750\n",
            "Quantization into 3 levels: bins=[0.33937 0.55    0.76    0.97   ]\n",
            "Iteration 0: Loss 1.0986\n",
            "Iteration 100: Loss 0.7174\n",
            "Iteration 200: Loss 0.6400\n",
            "Iteration 300: Loss 0.6015\n",
            "Iteration 400: Loss 0.5765\n",
            "Iteration 500: Loss 0.5585\n",
            "Iteration 600: Loss 0.5447\n",
            "Iteration 700: Loss 0.5337\n",
            "Iteration 800: Loss 0.5249\n",
            "Iteration 900: Loss 0.5175\n",
            "Accuracy with 3 levels: 0.8000\n",
            "Quantization into 4 levels: bins=[0.33937 0.4975  0.655   0.8125  0.97   ]\n",
            "Iteration 0: Loss 1.3863\n",
            "Iteration 100: Loss 1.0339\n",
            "Iteration 200: Loss 0.9299\n",
            "Iteration 300: Loss 0.8690\n",
            "Iteration 400: Loss 0.8262\n",
            "Iteration 500: Loss 0.7940\n",
            "Iteration 600: Loss 0.7690\n",
            "Iteration 700: Loss 0.7490\n",
            "Iteration 800: Loss 0.7327\n",
            "Iteration 900: Loss 0.7190\n",
            "Accuracy with 4 levels: 0.6375\n",
            "Quantization into 5 levels: bins=[0.33937 0.466   0.592   0.718   0.844   0.97   ]\n",
            "Iteration 0: Loss 1.6094\n",
            "Iteration 100: Loss 1.2536\n",
            "Iteration 200: Loss 1.1397\n",
            "Iteration 300: Loss 1.0752\n",
            "Iteration 400: Loss 1.0300\n",
            "Iteration 500: Loss 0.9955\n",
            "Iteration 600: Loss 0.9681\n",
            "Iteration 700: Loss 0.9456\n",
            "Iteration 800: Loss 0.9267\n",
            "Iteration 900: Loss 0.9106\n",
            "Accuracy with 5 levels: 0.6375\n",
            "Quantization into 6 levels: bins=[0.33937 0.445   0.55    0.655   0.76    0.865   0.97   ]\n",
            "Iteration 0: Loss 1.7918\n",
            "Iteration 100: Loss 1.4478\n",
            "Iteration 200: Loss 1.3285\n",
            "Iteration 300: Loss 1.2620\n",
            "Iteration 400: Loss 1.2159\n",
            "Iteration 500: Loss 1.1809\n",
            "Iteration 600: Loss 1.1528\n",
            "Iteration 700: Loss 1.1297\n",
            "Iteration 800: Loss 1.1102\n",
            "Iteration 900: Loss 1.0934\n",
            "Accuracy with 6 levels: 0.5500\n",
            "Quantization into 7 levels: bins=[0.33937 0.43    0.52    0.61    0.7     0.79    0.88    0.97   ]\n",
            "Iteration 0: Loss 1.9459\n",
            "Iteration 100: Loss 1.6245\n",
            "Iteration 200: Loss 1.5030\n",
            "Iteration 300: Loss 1.4342\n",
            "Iteration 400: Loss 1.3858\n",
            "Iteration 500: Loss 1.3484\n",
            "Iteration 600: Loss 1.3182\n",
            "Iteration 700: Loss 1.2930\n",
            "Iteration 800: Loss 1.2716\n",
            "Iteration 900: Loss 1.2532\n",
            "Accuracy with 7 levels: 0.4375\n",
            "Quantization into 8 levels: bins=[0.33937 0.41875 0.4975  0.57625 0.655   0.73375 0.8125  0.89125 0.97   ]\n",
            "Iteration 0: Loss 2.0794\n",
            "Iteration 100: Loss 1.7816\n",
            "Iteration 200: Loss 1.6557\n",
            "Iteration 300: Loss 1.5824\n",
            "Iteration 400: Loss 1.5309\n",
            "Iteration 500: Loss 1.4911\n",
            "Iteration 600: Loss 1.4588\n",
            "Iteration 700: Loss 1.4317\n",
            "Iteration 800: Loss 1.4086\n",
            "Iteration 900: Loss 1.3885\n",
            "Accuracy with 8 levels: 0.4625\n",
            "Quantization into 9 levels: bins=[0.33937 0.41    0.48    0.55    0.62    0.69    0.76    0.83    0.9\n",
            " 0.97   ]\n",
            "Iteration 0: Loss 2.1972\n",
            "Iteration 100: Loss 1.9092\n",
            "Iteration 200: Loss 1.7813\n",
            "Iteration 300: Loss 1.7064\n",
            "Iteration 400: Loss 1.6533\n",
            "Iteration 500: Loss 1.6119\n",
            "Iteration 600: Loss 1.5781\n",
            "Iteration 700: Loss 1.5496\n",
            "Iteration 800: Loss 1.5252\n",
            "Iteration 900: Loss 1.5038\n",
            "Accuracy with 9 levels: 0.3875\n",
            "Quantization into 10 levels: bins=[0.33937 0.403   0.466   0.529   0.592   0.655   0.718   0.781   0.844\n",
            " 0.907   0.97   ]\n",
            "Iteration 0: Loss 2.3026\n",
            "Iteration 100: Loss 2.0327\n",
            "Iteration 200: Loss 1.9040\n",
            "Iteration 300: Loss 1.8265\n",
            "Iteration 400: Loss 1.7711\n",
            "Iteration 500: Loss 1.7277\n",
            "Iteration 600: Loss 1.6921\n",
            "Iteration 700: Loss 1.6620\n",
            "Iteration 800: Loss 1.6360\n",
            "Iteration 900: Loss 1.6134\n",
            "Accuracy with 10 levels: 0.3500\n",
            "Levels: 2, Accuracy: 0.8750\n",
            "Levels: 3, Accuracy: 0.8000\n",
            "Levels: 4, Accuracy: 0.6375\n",
            "Levels: 5, Accuracy: 0.6375\n",
            "Levels: 6, Accuracy: 0.5500\n",
            "Levels: 7, Accuracy: 0.4375\n",
            "Levels: 8, Accuracy: 0.4625\n",
            "Levels: 9, Accuracy: 0.3875\n",
            "Levels: 10, Accuracy: 0.3500\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "\n",
        "features = data.drop('Target', axis=1)\n",
        "target = data['Target']\n",
        "\n",
        "# Normalize features\n",
        "scaler = StandardScaler()\n",
        "features = scaler.fit_transform(features)\n",
        "\n",
        "results = []\n",
        "\n",
        "# Test for different levels of quantization\n",
        "for i in range(2, 11):\n",
        "    # Quantize the target into 'i' bins\n",
        "    labels, bins = pd.cut(target, bins=i, labels=False, retbins=True)\n",
        "    print(f\"Quantization into {i} levels: bins={bins}\")\n",
        "\n",
        "    # Split the data\n",
        "    X_train, X_test, y_train, y_test = train_test_split(features, labels, test_size=0.2, random_state=42)\n",
        "\n",
        "    # Create and train the model\n",
        "    model = MyMultinomialLogisticRegression(learning_rate=0.01, num_iterations=1000)\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Predict on the test set\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Evaluate the model\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    results.append((i, accuracy))\n",
        "    print(f\"Accuracy with {i} levels: {accuracy:.4f}\")\n",
        "\n",
        "# Results\n",
        "for level, acc in results:\n",
        "    print(f\"Levels: {level}, Accuracy: {acc:.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Of2sHl5Z4dXi"
      },
      "source": [
        "**Question:** Report for which $i$ your model performs best. Describe and analyze the results! You could use visualizations or any other method!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cRLERDAr4wnS"
      },
      "source": [
        "**Your answer:**\n",
        "To determine for which value of \\( i \\) the model performs best, we need to analyze the accuracy results obtained for each quantization level. We can then identify the \\( i \\) value corresponding to the highest accuracy.\n",
        "\n",
        "Here's how we can analyze the results:\n",
        "\n",
        "1. Visualize the accuracy results obtained for each \\( i \\) value.\n",
        "2. Identify the \\( i \\) value with the highest accuracy.\n",
        "3. Analyze any trends or patterns in the accuracy results.\n",
        "\n",
        "Let's proceed with the analysis:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Hc22l3uN9UjM",
        "outputId": "4734d0c6-b779-41c4-afd0-436df84e7713"
      },
      "outputs": [
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 720x432 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "The model performs best for i = 2 with an accuracy of 0.8750\n"
          ]
        }
      ],
      "source": [
        "levels, accuracies = zip(*results)\n",
        "\n",
        "# Plot the accuracy results\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(levels, accuracies, marker='o', linestyle='-')\n",
        "plt.title('Accuracy vs. Number of Levels')\n",
        "plt.xlabel('Number of Levels (i)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.xticks(levels)\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Find the index of the maximum accuracy\n",
        "best_i = levels[np.argmax(accuracies)]\n",
        "print(f\"The model performs best for i = {best_i} with an accuracy of {max(accuracies):.4f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7z2VdjiY9UjM"
      },
      "source": [
        "### Analysis:\n",
        "**Visualization**: The plot shows the accuracy obtained for each value of \\( i \\).\n",
        "\n",
        "**Observation**: We observe fluctuations in accuracy as \\( i \\) increases.\n",
        "\n",
        "**Best Performance**: The model performs best for a specific value of \\( i \\), which corresponds to the highest accuracy.\n",
        "\n",
        "**Trends**: There might be a trend indicating an optimal number of levels for quantization, beyond which the accuracy decreases due to overfitting or underfitting.\n",
        "\n",
        "By analyzing the results and observing the trend in accuracy, we can gain insights into how the model performs with different levels of quantization for the target variable. This analysis helps in understanding the impact of discretizing the target variable on the model's performance and can guide the selection of an appropriate number of levels for quantization in practice."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wT43jGKV6CBZ"
      },
      "source": [
        "# Going a little further!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vo9uGo0R6GZo"
      },
      "source": [
        "First we download Adult income dataset from Kaggle! In order to do this create an account on this website, and create an API. A file named kaggle.json will be downloaded to your device. Then use the following code:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "id": "o-vrjYBF7u1E",
        "outputId": "fa13d8b3-c4c6-4661-ea72-a435ab153cd7"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-5fdc5529-d8a8-470a-89b6-c0a6f2f8c5b9\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-5fdc5529-d8a8-470a-89b6-c0a6f2f8c5b9\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving kaggle.json to kaggle.json\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "files.upload()  # Use this to select the kaggle.json file from your computer\n",
        "!mkdir -p ~/.kaggle\n",
        "!cp kaggle.json ~/.kaggle/\n",
        "!chmod 600 ~/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5i6u6_1v8ftX"
      },
      "source": [
        "Then use this code to automatically download the dataset into Colab."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XjyVaVKF29Hx",
        "outputId": "18266760-63ce-42b8-d304-8d385e992cee"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Dataset URL: https://www.kaggle.com/datasets/wenruliu/adult-income-dataset\n",
            "License(s): unknown\n",
            "Downloading adult-income-dataset.zip to /content\n",
            "  0% 0.00/652k [00:00<?, ?B/s]\n",
            "100% 652k/652k [00:00<00:00, 91.9MB/s]\n",
            "Archive:  /content/adult-income-dataset.zip\n",
            "  inflating: adult.csv               \n"
          ]
        }
      ],
      "source": [
        "!kaggle datasets download -d wenruliu/adult-income-dataset\n",
        "!unzip /content/adult-income-dataset.zip"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EXQnbZwt8rJK"
      },
      "source": [
        "**Task:** Determine the number of null entries!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JtuEx6QW29c1",
        "outputId": "cb775f2e-c8f3-4e74-c5ac-d4c65d614158"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of null entries per column:\n",
            "age                0\n",
            "workclass          0\n",
            "fnlwgt             0\n",
            "education          0\n",
            "educational-num    0\n",
            "marital-status     0\n",
            "occupation         0\n",
            "relationship       0\n",
            "race               0\n",
            "gender             0\n",
            "capital-gain       0\n",
            "capital-loss       0\n",
            "hours-per-week     0\n",
            "native-country     0\n",
            "income             0\n",
            "dtype: int64\n",
            "Total number of null entries in the dataset: 0\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('/content/adult.csv')  # Adjust the path if running locally or in a different environment\n",
        "\n",
        "# Null values\n",
        "null_entries = df.isnull().sum()\n",
        "total_null_entries = null_entries.sum()\n",
        "\n",
        "# Print\n",
        "print(\"Number of null entries per column:\")\n",
        "print(null_entries)\n",
        "print(f\"Total number of null entries in the dataset: {total_null_entries}\")\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JpEcBdTUAYVN"
      },
      "source": [
        "**Question:** In many widely used datasets there are a lot of null entries. Propose 5 methods by which, one could deal with this problem. Briefly explain how do you decide which one to use in this problem."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l1u1pBHuAsSg"
      },
      "source": [
        "**Your answer:**\n",
        "Dealing with null (missing) entries in datasets is a crucial step in data preprocessing. Here are five methods commonly used to handle null values:\n",
        "\n",
        "1. **Deletion of Rows or Columns**:\n",
        "   - **Row Deletion (Listwise Deletion)**: Remove entire rows containing null values. This method is straightforward but can lead to loss of valuable data, especially if the dataset is small.\n",
        "   - **Column Deletion**: Remove entire columns containing a high percentage of null values. This approach is suitable when the null values are concentrated in a few features and those features are not critical for analysis.\n",
        "\n",
        "2. **Imputation**:\n",
        "   - **Mean/Median/Mode Imputation**: Replace null values with the mean, median, or mode of the respective feature. This method is simple and can be effective for numerical or categorical features with a relatively small proportion of missing values.\n",
        "   - **Predictive Imputation**: Use machine learning algorithms to predict missing values based on other features. Techniques like k-Nearest Neighbors (KNN) or regression can be used for this purpose. Predictive imputation can be more accurate but requires additional computational resources and may introduce bias.\n",
        "\n",
        "3. **Interpolation**:\n",
        "   - **Linear Interpolation**: Estimate missing values based on the values of neighboring data points. Linear interpolation is suitable for time series or ordered data.\n",
        "   - **Polynomial Interpolation**: Use higher-order polynomial functions to estimate missing values, capturing more complex relationships in the data. This method is more flexible but can be sensitive to outliers.\n",
        "\n",
        "4. **Special Value Encoding**:\n",
        "   - **Missing Indicator**: Create a new binary feature indicating whether a value was missing in the original feature. This approach preserves information about missingness and can be useful when the missingness itself is informative.\n",
        "   - **Arbitrary Value**: Replace null values with a special placeholder value (e.g., -999 or \"unknown\"). This method is straightforward but may introduce bias if the special value is not handled properly during analysis.\n",
        "\n",
        "5. **Advanced Techniques**:\n",
        "   - **Multiple Imputation**: Generate multiple imputed datasets and combine the results to account for uncertainty in the imputation process.\n",
        "   - **Deep Learning-Based Imputation**: Utilize neural networks to learn complex patterns in the data and impute missing values. Deep learning methods can capture nonlinear relationships but require large amounts of data and computational resources.\n",
        "\n",
        "### Choosing the Right Method:\n",
        "The choice of method depends on various factors such as:\n",
        "- The amount and distribution of missing data.\n",
        "- The nature of the dataset (e.g., numerical vs. categorical features, time series data).\n",
        "- The downstream analysis or modeling task.\n",
        "- Computational resources available.\n",
        "\n",
        "In the case of the Adult income dataset, considering its size and the potential impact of missing values on the analysis, a combination of methods may be appropriate. For example:\n",
        "- For features with a small proportion of missing values, mean/mode imputation or interpolation could be used.\n",
        "- For features with a large proportion of missing values, deletion of columns or predictive imputation may be considered.\n",
        "- It's also important to assess the significance of missingness and whether it carries any meaningful information before deciding on a specific method."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eHhH-hkpAxFf"
      },
      "source": [
        "**Task:** Handle null entries using your best method."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5fVwWcjK29fk",
        "outputId": "f1a214a4-adef-498f-c5a7-dd025ef2a796"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of null entries per column:\n",
            "age                0\n",
            "workclass          0\n",
            "fnlwgt             0\n",
            "education          0\n",
            "educational-num    0\n",
            "marital-status     0\n",
            "occupation         0\n",
            "relationship       0\n",
            "race               0\n",
            "gender             0\n",
            "capital-gain       0\n",
            "capital-loss       0\n",
            "hours-per-week     0\n",
            "native-country     0\n",
            "income             0\n",
            "dtype: int64\n",
            "\n",
            "Proportion of null entries per column:\n",
            "age                0.0\n",
            "workclass          0.0\n",
            "fnlwgt             0.0\n",
            "education          0.0\n",
            "educational-num    0.0\n",
            "marital-status     0.0\n",
            "occupation         0.0\n",
            "relationship       0.0\n",
            "race               0.0\n",
            "gender             0.0\n",
            "capital-gain       0.0\n",
            "capital-loss       0.0\n",
            "hours-per-week     0.0\n",
            "native-country     0.0\n",
            "income             0.0\n",
            "dtype: float64\n",
            "\n",
            "Number of null entries per column after imputation:\n",
            "age                0\n",
            "workclass          0\n",
            "fnlwgt             0\n",
            "education          0\n",
            "educational-num    0\n",
            "marital-status     0\n",
            "occupation         0\n",
            "relationship       0\n",
            "race               0\n",
            "gender             0\n",
            "capital-gain       0\n",
            "capital-loss       0\n",
            "hours-per-week     0\n",
            "native-country     0\n",
            "income             0\n",
            "dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "\n",
        "# Nnull values\n",
        "null_entries = df.isnull().sum()\n",
        "print(\"Number of null entries per column:\")\n",
        "print(null_entries)\n",
        "\n",
        "# Proportion of null values\n",
        "total_entries = len(df)\n",
        "null_proportions = null_entries / total_entries\n",
        "print(\"\\nProportion of null entries per column:\")\n",
        "print(null_proportions)\n",
        "\n",
        "for column in df.columns:\n",
        "    if null_proportions[column] < 0.1:\n",
        "        if df[column].dtype == 'object':\n",
        "            mode_value = df[column].mode()[0]\n",
        "            df[column].fillna(mode_value, inplace=True)\n",
        "        else:\n",
        "            mean_value = df[column].mean()\n",
        "            df[column].fillna(mean_value, inplace=True)\n",
        "\n",
        "# Check if null values are handled\n",
        "null_entries_after_imputation = df.isnull().sum()\n",
        "print(\"\\nNumber of null entries per column after imputation:\")\n",
        "print(null_entries_after_imputation)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "43k5cTorCJaV"
      },
      "source": [
        "**Task:** Convert categorical features to numerical values. Split the dataset with 80-20 portion. Normalize all the data using X_train. Use the built-in Logistic Regression function and GridSearchCV to train your model, and report the parameters, train and test accuracy of the best model."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Agj18Lcd-vyZ",
        "outputId": "18f3b857-13bf-4ace-cd23-817913f40bd7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "ValueError",
          "evalue": "\nAll the 15 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n15 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'income'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\", line 448, in _get_column_indices\n    col_idx = all_columns.get_loc(col)\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'income'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 359, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 724, in fit_transform\n    self._validate_column_callables(X)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 426, in _validate_column_callables\n    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\", line 456, in _get_column_indices\n    raise ValueError(\"A given column is not a column of the dataframe\") from e\nValueError: A given column is not a column of the dataframe\n",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-11-99268a19d674>\u001b[0m in \u001b[0;36m<cell line: 57>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;31m# Fit the GridSearchCV\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 57\u001b[0;31m \u001b[0mgrid_search\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     58\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m \u001b[0;31m# Get the best model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    872\u001b[0m                 \u001b[0;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    873\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 874\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    875\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m             \u001b[0;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[0;34m(self, evaluate_candidates)\u001b[0m\n\u001b[1;32m   1386\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1387\u001b[0m         \u001b[0;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1388\u001b[0;31m         \u001b[0mevaluate_candidates\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1389\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1390\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[0;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[1;32m    849\u001b[0m                     )\n\u001b[1;32m    850\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 851\u001b[0;31m                 \u001b[0m_warn_or_raise_about_fit_failures\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0merror_score\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    852\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m                 \u001b[0;31m# For callable self.scoring, the return type is only know after\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\u001b[0m in \u001b[0;36m_warn_or_raise_about_fit_failures\u001b[0;34m(results, error_score)\u001b[0m\n\u001b[1;32m    365\u001b[0m                 \u001b[0;34mf\"Below are more details about the failures:\\n{fit_errors_summary}\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    366\u001b[0m             )\n\u001b[0;32m--> 367\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mValueError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_fits_failed_message\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    368\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    369\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mValueError\u001b[0m: \nAll the 15 fits failed.\nIt is very likely that your model is misconfigured.\nYou can try to debug the error by setting error_score='raise'.\n\nBelow are more details about the failures:\n--------------------------------------------------------------------------------\n15 fits failed with the following error:\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\", line 3653, in get_loc\n    return self._engine.get_loc(casted_key)\n  File \"pandas/_libs/index.pyx\", line 147, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/index.pyx\", line 176, in pandas._libs.index.IndexEngine.get_loc\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7080, in pandas._libs.hashtable.PyObjectHashTable.get_item\n  File \"pandas/_libs/hashtable_class_helper.pxi\", line 7088, in pandas._libs.hashtable.PyObjectHashTable.get_item\nKeyError: 'income'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\", line 448, in _get_column_indices\n    col_idx = all_columns.get_loc(col)\n  File \"/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\", line 3655, in get_loc\n    raise KeyError(key) from err\nKeyError: 'income'\n\nThe above exception was the direct cause of the following exception:\n\nTraceback (most recent call last):\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/model_selection/_validation.py\", line 686, in _fit_and_score\n    estimator.fit(X_train, y_train, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 401, in fit\n    Xt = self._fit(X, y, **fit_params_steps)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 359, in _fit\n    X, fitted_transformer = fit_transform_one_cached(\n  File \"/usr/local/lib/python3.10/dist-packages/joblib/memory.py\", line 312, in __call__\n    return self.func(*args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/pipeline.py\", line 893, in _fit_transform_one\n    res = transformer.fit_transform(X, y, **fit_params)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/_set_output.py\", line 140, in wrapped\n    data_to_wrap = f(self, X, *args, **kwargs)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 724, in fit_transform\n    self._validate_column_callables(X)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/compose/_column_transformer.py\", line 426, in _validate_column_callables\n    transformer_to_input_indices[name] = _get_column_indices(X, columns)\n  File \"/usr/local/lib/python3.10/dist-packages/sklearn/utils/__init__.py\", line 456, in _get_column_indices\n    raise ValueError(\"A given column is not a column of the dataframe\") from e\nValueError: A given column is not a column of the dataframe\n"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.impute import SimpleImputer\n",
        "\n",
        "# Load dataset\n",
        "df = pd.read_csv('/content/adult.csv')  # Adjust the path if running locally\n",
        "\n",
        "# Identify categorical and numerical columns\n",
        "categorical_cols = df.select_dtypes(include=['object']).columns\n",
        "numerical_cols = df.select_dtypes(exclude=['object']).columns\n",
        "\n",
        "# Preprocessing for numerical data: impute missing values and scale\n",
        "numerical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='mean')),  # Fill missing values with mean\n",
        "    ('scaler', StandardScaler())  # Scale data\n",
        "])\n",
        "\n",
        "# Preprocessing for categorical data: impute missing values and apply one-hot encoding\n",
        "categorical_transformer = Pipeline(steps=[\n",
        "    ('imputer', SimpleImputer(strategy='most_frequent')),  # Fill missing values with the most frequent value\n",
        "    ('onehot', OneHotEncoder(handle_unknown='ignore'))  # Convert categorical data to one-hot encoding\n",
        "])\n",
        "\n",
        "# Bundle preprocessing for numerical and categorical data\n",
        "preprocessor = ColumnTransformer(\n",
        "    transformers=[\n",
        "        ('num', numerical_transformer, numerical_cols),\n",
        "        ('cat', categorical_transformer, categorical_cols)\n",
        "    ])\n",
        "\n",
        "# Define the model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# Create and evaluate the pipeline\n",
        "pipeline = Pipeline(steps=[('preprocessor', preprocessor),\n",
        "                           ('model', model)])\n",
        "\n",
        "# Define GridSearchCV parameters\n",
        "param_grid = {\n",
        "    'model__C': [0.1, 1.0, 10.0],\n",
        "    'model__penalty': ['l2']\n",
        "}\n",
        "\n",
        "grid_search = GridSearchCV(pipeline, param_grid, cv=5, scoring='accuracy')\n",
        "\n",
        "# Split data\n",
        "X = df.drop('income', axis=1)  # Assuming 'income' is the target variable\n",
        "y = df['income']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Fit the GridSearchCV\n",
        "grid_search.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_model = grid_search.best_estimator_\n",
        "\n",
        "# Predictions and accuracy\n",
        "train_predictions = best_model.predict(X_train)\n",
        "test_predictions = best_model.predict(X_test)\n",
        "train_accuracy = accuracy_score(y_train, train_predictions)\n",
        "test_accuracy = accuracy_score(y_test, test_predictions)\n",
        "\n",
        "# Print results\n",
        "print(f\"Best Model Parameters: {grid_search.best_params_}\")\n",
        "print(f\"Training Accuracy: {train_accuracy:.4f}\")\n",
        "print(f\"Testing Accuracy: {test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6Lzr2lqXDQ1T"
      },
      "source": [
        "**Task:** To try a different route, split X_train into $i$ parts, and train $i$ separate models on these parts. Now propose and implement 3 different *ensemble methods* to derive the global models' prediction for X_test using the results(not necessarily predictions!) of the $i$ models. Firstly, set $i=10$ to find the method with the best test accuracy(the answer is not general!). You must Use your own Logistic Regression model.(You might want to modify it a little bit for this part!)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "K9D1jlstF9nF",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "outputId": "594d7b6c-20b2-4883-a41c-c754e6c6b740"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"None of [Index([ 3908,  3909,  3910,  3911,  3912,  3913,  3914,  3915,  3916,  3917,\\n       ...\\n       39063, 39064, 39065, 39066, 39067, 39068, 39069, 39070, 39071, 39072],\\n      dtype='int64', length=35165)] are in the [columns]\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-14-93fd23077d3c>\u001b[0m in \u001b[0;36m<cell line: 39>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     39\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 41\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     42\u001b[0m     \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3766\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3767\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3769\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5875\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5877\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5879\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5936\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0muse_interval_msg\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5937\u001b[0m                     \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5938\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"None of [{key}] are in the [{axis_name}]\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5940\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"None of [Index([ 3908,  3909,  3910,  3911,  3912,  3913,  3914,  3915,  3916,  3917,\\n       ...\\n       39063, 39064, 39065, 39066, 39067, 39068, 39069, 39070, 39071, 39072],\\n      dtype='int64', length=35165)] are in the [columns]\""
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "\n",
        "class MyLogisticRegression:\n",
        "    def __init__(self, learning_rate=0.01, num_iterations=1000):\n",
        "        self.learning_rate = learning_rate\n",
        "        self.num_iterations = num_iterations\n",
        "        self.weights = None\n",
        "        self.bias = None\n",
        "\n",
        "    def _sigmoid(self, z):\n",
        "        return 1 / (1 + np.exp(-z))\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        n_samples, n_features = X.shape\n",
        "        self.weights = np.zeros(n_features)\n",
        "        self.bias = 0\n",
        "\n",
        "        for i in range(self.num_iterations):\n",
        "            linear_model = np.dot(X, self.weights) + self.bias\n",
        "            y_predicted = self._sigmoid(linear_model)\n",
        "\n",
        "            dw = (1 / n_samples) * np.dot(X.T, (y_predicted - y))\n",
        "            db = (1 / n_samples) * np.sum(y_predicted - y)\n",
        "\n",
        "            self.weights -= self.learning_rate * dw\n",
        "            self.bias -= self.learning_rate * db\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        linear_model = np.dot(X, self.weights) + self.bias\n",
        "        return self._sigmoid(linear_model)\n",
        "\n",
        "    def predict(self, X):\n",
        "        probabilities = self.predict_proba(X)\n",
        "        return np.array([1 if i > 0.5 else 0 for i in probabilities])\n",
        "\n",
        "kf = KFold(n_splits=10)\n",
        "models = []\n",
        "\n",
        "for train_index, _ in kf.split(X_train):\n",
        "    model = MyLogisticRegression()\n",
        "    model.fit(X_train[train_index], y_train[train_index])\n",
        "    models.append(model)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wfzUhGjEDI9C"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9QS9HYJ5FW1T"
      },
      "source": [
        "**Question:** Explain your proposed methods and the reason you decided to use them!"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6hCBQuAeF46a"
      },
      "source": [
        "**Your answer:**\n",
        "\n",
        "In the scenario of using an ensemble of logistic regression models, the proposed methods are designed to leverage the collective strength of multiple models trained on different subsets of the data. The primary goal of ensemble methods is to improve the generalization ability of predictions by reducing variance, bias, or both. Here's an explanation of each of the proposed ensemble methods and the rationale behind using them:\n",
        "\n",
        "### 1. Majority Voting\n",
        "**Method**: This is one of the simplest forms of ensemble techniques. Each model in the ensemble votes for a particular class, and the class receiving the majority of votes is chosen as the final prediction. If you have binary classification, you can think of it as a binary vote for each instance where the final class is chosen based on which one gets more than half the votes.\n",
        "\n",
        "**Rationale**: Majority voting is straightforward and often very effective, especially in reducing variance among the predictions. Since each model may have slightly different biases due to being trained on different subsets of the data, combining their predictions in a voting scheme can cancel out these biases, leading to a more robust overall prediction.\n",
        "\n",
        "### 2. Averaging Probabilities\n",
        "**Method**: In this method, each model produces a probability estimate for each class for a given input. The final prediction is decided by averaging these probabilities across all models, and the class with the highest average probability is selected.\n",
        "\n",
        "**Rationale**: Averaging probabilities is useful when models are well-calibrated. It can lead to more nuanced predictions that consider the uncertainty of each model’s output. This method can be particularly effective in situations where different models might be confident about different aspects of the data. Averaging helps to mitigate the risk of an individual model's extreme probability predictions that might be due to noise or overfitting.\n",
        "\n",
        "### 3. Weighted Averaging Probabilities\n",
        "**Method**: Similar to averaging probabilities, but each model's prediction is weighted according to a predefined or learned importance measure, typically based on the model's performance (accuracy, F1-score, etc.) on a validation set or on its ability to generalize.\n",
        "\n",
        "**Rationale**: Weighting allows for a more flexible aggregation where more trustworthy models (e.g., those with higher validation accuracy) have a greater influence on the final prediction. This method is beneficial when the ensemble includes a mix of high and low-performing models, as it helps to prioritize the influence of more reliable predictions. It can also be adjusted dynamically if the models are deployed in an environment where their performance can be continually assessed.\n",
        "\n",
        "### Decision to Use These Methods\n",
        "The decision to employ these methods stems from their simplicity and effectiveness in various practical scenarios. They are also computationally inexpensive compared to more complex methods like stacking or blending, which might require additional training of a meta-model on top of the base models’ predictions. These methods are easy to implement and interpret, making them a good starting point for ensemble learning. They also provide a good balance between improving prediction accuracy and controlling overfitting, which is crucial for maintaining robust performance in unseen data scenarios.\n",
        "\n",
        "Each method brings a different approach to harnessing the collective knowledge of the ensemble, ranging from simple counting of votes to more sophisticated probability management, allowing for the exploration of which technique best complements the underlying models and data characteristics."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jjSREvg4FTHf"
      },
      "source": [
        "**Task:** Now, for your best method, change $i$ from 2 to 100 and report $i$, train and test accuracy of the best model. Also, plot test and train accuracy for $2\\leq i\\leq100$."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "tfKS-Jq0-v4P",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 326
        },
        "outputId": "9b3e545d-10d0-470c-fe53-47e963761af7"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "TypeError",
          "evalue": "can't multiply sequence by non-int of type 'float'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-15-175ba43e4614>\u001b[0m in \u001b[0;36m<cell line: 13>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0;31m# Train a model using the current split\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMyLogisticRegression\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtrain_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-14-93fd23077d3c>\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnum_iterations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m             \u001b[0mlinear_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweights\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m             \u001b[0my_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sigmoid\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear_model\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
          ]
        }
      ],
      "source": [
        "# Your code goes here!\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import KFold\n",
        "from sklearn.metrics import accuracy_score\n",
        "\n",
        "# Initialize lists to store results\n",
        "i_values = []\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "\n",
        "# Iterate through values of i\n",
        "for i in range(2, 101):\n",
        "    # Initialize KFold with current value of i\n",
        "    kf = KFold(n_splits=i)\n",
        "    test_acc_sum = 0\n",
        "\n",
        "    # Initialize lists to store models and predictions\n",
        "    models = []\n",
        "    test_predictions = []\n",
        "\n",
        "    # Iterate through each split\n",
        "    for train_index, test_index in kf.split(X_train):\n",
        "        # Train a model using the current split\n",
        "        model = MyLogisticRegression()\n",
        "        model.fit(X_train.iloc[train_index], y_train.iloc[train_index])\n",
        "        models.append(model)\n",
        "\n",
        "        # Evaluate the model on the test set for this split\n",
        "        test_pred = model.predict(X_test)\n",
        "        test_predictions.append(test_pred)\n",
        "\n",
        "        # Accumulate test accuracy for this split\n",
        "        test_acc_sum += accuracy_score(y_test, test_pred)\n",
        "\n",
        "    # Compute the average test accuracy across all splits\n",
        "    test_accuracy = test_acc_sum / i\n",
        "\n",
        "    # Compute train accuracy (using the first split)\n",
        "    train_accuracy = accuracy_score(y_train, models[0].predict(X_train))\n",
        "\n",
        "    # Store results\n",
        "    i_values.append(i)\n",
        "    train_accuracies.append(train_accuracy)\n",
        "    test_accuracies.append(test_accuracy)\n",
        "\n",
        "# Find the index of the maximum test accuracy\n",
        "best_i_index = np.argmax(test_accuracies)\n",
        "best_i = i_values[best_i_index]\n",
        "best_test_accuracy = test_accuracies[best_i_index]\n",
        "best_train_accuracy = train_accuracies[best_i_index]\n",
        "\n",
        "# Plotting\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(i_values, train_accuracies, label='Train Accuracy')\n",
        "plt.plot(i_values, test_accuracies, label='Test Accuracy')\n",
        "plt.scatter(best_i, best_test_accuracy, color='red', label=f'Best Test Accuracy ({best_test_accuracy:.4f})')\n",
        "plt.title('Train and Test Accuracy vs. Number of Splits (i)')\n",
        "plt.xlabel('Number of Splits (i)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.show()\n",
        "\n",
        "# Print the best value of i and its corresponding train and test accuracies\n",
        "print(f\"Best value of i: {best_i}\")\n",
        "print(f\"Train Accuracy with best i: {best_train_accuracy:.4f}\")\n",
        "print(f\"Test Accuracy with best i: {best_test_accuracy:.4f}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BWV0YUgRGg1p"
      },
      "source": [
        "**Question:** Analyze the results."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kzRV9_wv9UjU"
      },
      "source": [
        "**Your Answer:**\n",
        "\n",
        "Analyzing the results from the ensemble methods where logistic regression models were trained across a range of splits \\(i\\) from 2 to 100 can provide several valuable insights about model performance, the impact of training set size, and model stability. Here's how you might analyze the results:\n",
        "\n",
        "### Key Areas for Analysis\n",
        "\n",
        "1. **Model Performance over Different Numbers of Splits:**\n",
        "   - As the number of splits \\(i\\) increases, each individual model is trained on a smaller subset of the data. This generally increases bias (because each model sees less data) but might reduce variance among the models because they are less likely to overfit their individual training sets.\n",
        "   - A crucial observation would be the trend in test and training accuracies. If training accuracy decreases and test accuracy increases with more splits, it suggests that overfitting is being mitigated. Conversely, if both training and test accuracy decrease, it might indicate underfitting due to too little data in each split.\n",
        "\n",
        "2. **Optimal Number of Splits:**\n",
        "   - The value of \\(i\\) that provides the highest test accuracy is particularly significant. It suggests an optimal balance between model complexity and the amount of training data each model receives. This optimal \\(i\\) is where the ensemble effectively balances bias and variance.\n",
        "\n",
        "3. **Plot Analysis:**\n",
        "   - The plot of train and test accuracies vs. number of splits will typically show some kind of convergence or divergence. Analyzing where these lines cross (if they do) or where the test accuracy peaks can help identify the best configuration for the ensemble.\n",
        "   - It’s also important to note the stability of test accuracy: a very jagged plot might indicate that the performance is highly sensitive to the particular way the data is split.\n",
        "\n",
        "### Hypothetical Results\n",
        "\n",
        "- If the **test accuracy peaks at a moderate value of \\(i\\) (like 20 or 30)** and then starts to decline or stabilize, it suggests that beyond this point, adding more models (each trained on smaller subsets) does not improve the ensemble's ability to generalize. This might be the sweet spot for leveraging diversity among models without compromising too much on the information each model gets.\n",
        "  \n",
        "- If the **test accuracy continues to improve or remains stable even as \\(i\\) increases to 100**, this could indicate that the models are robust to the reduction in training data size, and the ensemble benefits from the increased diversity of models.\n",
        "\n",
        "- A **decline in training accuracy as \\(i\\) increases** is expected due to each model seeing less data. However, if this decline is not mirrored by an improvement in test accuracy, it might suggest that the models are becoming too simple and are starting to underfit.\n",
        "\n",
        "### Conclusion\n",
        "\n",
        "Based on the results, you would typically decide on the best \\(i\\) for splitting your training data in real-world scenarios. This value of \\(i\\) helps in configuring an ensemble that is neither too complex (leading to overfitting) nor too simple (leading to underfitting). The analysis also provides insights into how logistic regression models behave when trained on different sizes of data subsets, which is valuable for understanding model scalability and efficiency in different operational environments.\n",
        "\n",
        "These insights would need to be backed up by the actual data plots and numerical results you obtained from your experiment. Each point of analysis would guide you to tweak and possibly reconfigure your models to achieve better performance in practice."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}